{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20 Prep Part 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN0YdGZpTINl6BM4wsnhD+W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-forty-two/DataSetsForML/blob/master/20_Prep_Part_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igtRtQAiDEQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RNN v/s CNN\n",
        "\n",
        "# RNNs: Recurrent Layers -> Output becomes part of input (Electrical feedback loops). This may\n",
        "# happen on any type of layer. \n",
        "# y = f(x,y)\n",
        "# Sequence -> multiple items taken in a segment for pattern recog\n",
        "# Understanding patterns makes it highly overfitted towards data that happens in sequence\n",
        "# like audio(beats or tune), speech, NLP (word2vector), CAPTCHA (images happening again and again), TIme Series\n",
        "# Examples: LSTM, GRU\n",
        "# practical examples: time series analysis, translation, encoding/decoding, encry/dec,\n",
        "# word prediction, autocorrection, sentence completion, customer support-> escalation \n",
        "# genetic sequence detection, disease patterns, cell structure (nucleus and cytoplasm density)\n",
        "# \n",
        "# CNNs: Recurring Layers -> happening again and again -> partially connected layers that \n",
        "# learn pattern with help of filtering. Filter (Convolution) is multiplied into the input set \n",
        "# to generate Activation Map. This activation map serves as feature for the next layer.\n",
        "# y2 = f(y1(x))\n",
        "# Filtering -> Convolutions are multiplied into the data and passed through a 'relu' activation \n",
        "# Understanding minutest details resulting on the Activation maps, so that if initial object\n",
        "# was distorted, even those features are understood \n",
        "# Like: object detection, movement detection, computer vision, emotion analysis on NLP\n",
        "# Examples: ResNet, VggNet, AlexNet\n",
        "# practical examples: object detection, activity detection, finger/facial recognition\n",
        "# conveyer belts/smart car/security cameras, Text extraction from images \n",
        "\n",
        "# Fully Connected Layers-> Dense Layers\n",
        "# Dense layers are usually used towards input or output because while they are great at pattern\n",
        "# detection, they also are very expensive, hence their usage should be controlled via activation\n",
        "# functions inside the layer\n",
        "# Output layers are usually dense, but can be a different layer also, depending on your labels\n",
        "# Multi-Categorical output-> Dense('softmax')\n",
        "# Binary-Categorical output-> Dense('Sigmoid')\n",
        "# Regression (only pos output, like Age, items sold)-> Dense('ReLU')\n",
        "# Regression (neg acceptable, profit/loss, temperature)-> Dense (no activation)\n",
        "# Probability-> Dense('Sigmoid')\n",
        "# Value between -1 to +1 -> Dense ('tanh')\n",
        "\n",
        "\n",
        "# input layer is decided based on your data. \n",
        "# Images -> Flattening \n",
        "# Text -> Embedding \n",
        "# Numbers -> Dense \n",
        "\n",
        "# Partially Connected Layers-> Convolutional Layers\n",
        "# Convolutional Layers are usually inside hidden layers to convolute and filtrer on FEATURES\n",
        "# generated post data augmentation. Here, activation functions help with filtering and changing\n",
        "# scale of the data. Activation Fns are usually AFTER Conv layers, not inside. \n",
        "# Because they are not fully connected, long chains of Convolutional layers are required to \n",
        "# learn finer and more complex pattern with each block of Conv layer \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Can be used in combination also (HIGHLY EXPERIMENTAL)\n",
        "# https://www.datasciencecentral.com/profiles/blogs/combining-cnns-and-rnns-crazy-or-genius\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}