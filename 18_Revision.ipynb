{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "18 Revision.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMpqCVR9mV6KhiolGPYmwb2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-forty-two/DataSetsForML/blob/master/18_Revision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNDIGh018NZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLzTa7Xl8P9D",
        "colab_type": "text"
      },
      "source": [
        "https://machinelearningmastery.com/best-advice-for-configuring-backpropagation-for-deep-learning-neural-networks/\n",
        "\n",
        "\n",
        "Bias  -> an inbuilt mistake that will always happen\n",
        " \n",
        "Variance -> ability to vary \n",
        "\n",
        "Hypothetical Relationship between Variance and Bias\n",
        "\n",
        "f(V) + f(B) = 1 \n",
        "\n",
        "If variance increases, bias should decrease\n",
        "\n",
        "if bias increases, variance should decrease \n",
        "\n",
        "Tree: a tree's learning ability is it's depth. \n",
        "\n",
        "If you are close to the node, you are closer to input variables.\n",
        "And the result, will be BIASED towards those input variables.\n",
        "\n",
        "if we go down a tree, we are further from input. This increase \n",
        "variance. And thus bias is reduced. \n",
        "\n",
        "Neither. I don't want too much variance- if my answers are varying\n",
        "too much, then chances of wrong predictions increase \n",
        "\n",
        "If i have too much bias- my chances of underfitting increace \n",
        "If I have too much Variance - my answers are too random \n",
        "\n",
        "Variance   Bias     Result \n",
        "Low        High     Underfitting\n",
        "High       Low      Overfitting\n",
        "Low        Low      Good Model \n",
        "High       High     Goofy, stay away \n",
        "\n",
        "\n",
        "\n",
        "y = weights*input + bias <- bias adjust can lead to overfitting \n",
        "\n",
        "\n",
        "Bias affects your input severely in this case because y->f(bias)\n",
        "\n",
        "if bias increases, keeping y constant, weights*input should decrease\n",
        "input is also constant, thus only weights can decrease\n",
        "\n",
        "if weights become too less without much variation\n",
        "it was poor learning anyway \n",
        "\n",
        "We need weights variable. \n",
        "\n",
        "On the other hand, if weights were TOO variable, then even with \n",
        "bias, I would get a inconsistent performance and loss\n",
        "\n",
        "It's a gamble between finding perfect values. And that's where\n",
        "Gradient Descent comes into picture. \n",
        "\n",
        "It does a 3D mapping of weights, biases and any other factors \n",
        "impacting my loss metrics. And then using GD (either stoch. or batch)\n",
        "we find local or global minimas. This gives us the best values\n",
        "( or workable values) of weights and biases thus solving our\n",
        "bias-variance tradeoff. \n",
        "\n",
        "Tree-> manage it by yourself, write a function for it\n",
        "\n",
        "NN -> SGD or BGD done by NN in back-prop\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmefoOQM9Bwo",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}