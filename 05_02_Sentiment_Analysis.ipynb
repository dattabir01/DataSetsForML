{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05 02 Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNnKJkFoKXpaqpx1pWTu1x6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-forty-two/DataSetsForML/blob/master/05_02_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odFdaNTIXCB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YOu can write your own loss and metrics or optimizer functions\n",
        "# imdb -> lot of movie reviews\n",
        "# tensorflow's keras -> has a dirty version of imdb \n",
        "# it is dirty because of improper data cleaning\n",
        "# so we will need to adjust it manually # this is a problem with this specific version of tf only\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTf_rxCQXr1j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "40759171-63df-4de0-ff8b-1400e08f6bb7"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJxfICxRX0tk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imdb = keras.datasets.imdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b5gm405X443",
        "colab_type": "code",
        "outputId": "b105ae8a-3217-4e01-b1e3-19c34c8d5f67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#dir(imdb)\n",
        "#imdb.__dict__\n",
        "HP_dictionary_size = 10000\n",
        "(trainx, trainy),(testx,testy) = imdb.load_data(num_words=HP_dictionary_size)\n",
        "\n",
        "# EnGLISH "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxoS9mOyX-Bc",
        "colab_type": "code",
        "outputId": "1349daae-c595-47f2-d3ca-a1441717e29d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#trainx[0]\n",
        "#print(len(testx))\n",
        "# since we have so much data, we will use some of it for validation \n",
        "# SUBSET-> every time the epoch is complete, it will validate the result against validation data also. \n",
        "# after entire cycle is done, only then testing data will be predicted\n",
        "# much better educated guess\n",
        "# trainx-> 25,000\n",
        "# test-> 15,000\n",
        "# vali -> 10,000\n",
        "#print(decoder(trainx[2]))\n",
        "trainy[:20]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AksD_4F6Y5EV",
        "colab_type": "code",
        "outputId": "bc0a83ff-03af-42a6-f542-c76d0fbd40df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "trainx[0] # an english sentence has been converted to numbers for us\n",
        "word_index = imdb.get_word_index()\n",
        "for word,i in word_index.items():\n",
        "  print(i)\n",
        "  print(word)\n",
        "  if int(i)>30:\n",
        "    break\n",
        "\n",
        "# DICTIONARY-> UNORDERED , INDEXED "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "34701\n",
            "fawn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBjLtR5wZYo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bunch of data cleaning that happens \n",
        "# certain rules that every NLP engineers follow\n",
        "# imdb dataset-> these rules were missed \n",
        "# we place certain special words inside our regular data which we know users will not type\n",
        "# and we use those words to identify the structure of our string\n",
        "# <START> -> tells you beginning of the data \n",
        "# i have taken only 10,000 words. But in sentiments, there will more than that, such words are marked\n",
        "# as unknowns! <UNK> \n",
        "# <PAD> 0       WHEREVER WE WANT THE WEIGHT OF A WORD TO BE 0-> it should have no impact on learning. w1*<PAD>=0\n",
        "# <START> 1     Indicates starting of a input data\n",
        "# <UNK> 2       Unknowns, exist in dictionary but we are not considering them\n",
        "# <UNUSED> 3    developers can use this in their own context\n",
        "# ONLT FOR THIS CASE, we are doing this manually\n",
        "# otherwise, expect this to be done as a part of data cleaning \n",
        "# but there would be existing values on 0,1,2,3!!!!\n",
        "# before inserting, shift these values\n",
        "# Our indexing starts for 0, but this dataset indexes for 1\n",
        "# SENTENCE ->dict(sentence) -> NUMBERS [23 24 24 42 42 1012]\n",
        "word_index = {k:(v+3) for k,v in word_index.items()} # word_index <- word, get a number\n",
        "word_index[\"<PAD>\"]=0 # lambdas, list comprehensions -> DICTIONARY comprehension\n",
        "word_index[\"<START>\"]=1\n",
        "word_index[\"<UNK>\"]=2\n",
        "word_index[\"<UNUSED>\"]=3\n",
        "dictionary = {v:k for k,v in word_index.items()} #reverse-> i want to pass number and get word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dy-NdRGciU2",
        "colab_type": "code",
        "outputId": "7a4d7931-7f22-446c-a518-8f179e13799f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "for i,word in dictionary.items():\n",
        "  print(i)\n",
        "  print(word)\n",
        "  if int(i)>30:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "34704\n",
            "fawn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLyzfU6Ccpk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decoder(text):\n",
        "  decoded_text = [dictionary.get(word_index) for word_index in text]\n",
        "  sentence = ' '.join(decoded_text)\n",
        "  return sentence # en_gb, en_us, en_in, hi_in "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPDnufKki067",
        "colab_type": "code",
        "outputId": "912e2c04-56ad-4de6-8fda-08d892265b1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        }
      },
      "source": [
        "sentiment = {1:'Positive',0:'Negative'}\n",
        "for i in range(20):\n",
        "  print(decoder(trainx[i]))\n",
        "  print(sentiment.get(trainy[i]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n",
            "Positive\n",
            "<START> big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are sickening and funny in equal <UNK> the hair is big lots of boobs <UNK> men wear those cut <UNK> shirts that show off their <UNK> sickening that men actually wore them and the music is just <UNK> trash that plays over and over again in almost every scene there is trashy music boobs and <UNK> taking away bodies and the gym still doesn't close for <UNK> all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then\n",
            "Negative\n",
            "<START> this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had <UNK> working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how <UNK> this is to watch save yourself an hour a bit of your life\n",
            "Negative\n",
            "<START> the <UNK> <UNK> at storytelling the traditional sort many years after the event i can still see in my <UNK> eye an elderly lady my friend's mother retelling the battle of <UNK> she makes the characters come alive her passion is that of an eye witness one to the events on the <UNK> heath a mile or so from where she lives br br of course it happened many years before she was born but you wouldn't guess from the way she tells it the same story is told in bars the length and <UNK> of scotland as i discussed it with a friend one night in <UNK> a local cut in to give his version the discussion continued to closing time br br stories passed down like this become part of our being who doesn't remember the stories our parents told us when we were children they become our invisible world and as we grow older they maybe still serve as inspiration or as an emotional <UNK> fact and fiction blend with <UNK> role models warning stories <UNK> magic and mystery br br my name is <UNK> like my grandfather and his grandfather before him our protagonist introduces himself to us and also introduces the story that stretches back through generations it produces stories within stories stories that evoke the <UNK> wonder of scotland its rugged mountains <UNK> in <UNK> the stuff of legend yet <UNK> is <UNK> in reality this is what gives it its special charm it has a rough beauty and authenticity <UNK> with some of the finest <UNK> singing you will ever hear br br <UNK> <UNK> visits his grandfather in hospital shortly before his death he burns with frustration part of him <UNK> to be in the twenty first century to hang out in <UNK> but he is raised on the western <UNK> among a <UNK> speaking community br br yet there is a deeper conflict within him he <UNK> to know the truth the truth behind his <UNK> ancient stories where does fiction end and he wants to know the truth behind the death of his parents br br he is pulled to make a last <UNK> journey to the <UNK> of one of <UNK> most <UNK> mountains can the truth be told or is it all in stories br br in this story about stories we <UNK> bloody battles <UNK> lovers the <UNK> of old and the sometimes more <UNK> <UNK> of accepted truth in doing so we each connect with <UNK> as he lives the story of his own life br br <UNK> the <UNK> <UNK> is probably the most honest <UNK> and genuinely beautiful film of scotland ever made like <UNK> i got slightly annoyed with the <UNK> of hanging stories on more stories but also like <UNK> i <UNK> this once i saw the <UNK> picture ' forget the box office <UNK> of braveheart and its like you might even <UNK> the <UNK> famous <UNK> of the wicker man to see a film that is true to scotland this one is probably unique if you maybe <UNK> on it deeply enough you might even re <UNK> the power of storytelling and the age old question of whether there are some truths that cannot be told but only experienced\n",
            "Positive\n",
            "<START> worst mistake of my life br br i picked this movie up at target for 5 because i figured hey it's sandler i can get some cheap laughs i was wrong completely wrong mid way through the film all three of my friends were asleep and i was still suffering worst plot worst script worst movie i have ever seen i wanted to hit my head up against a wall for an hour then i'd stop and you know why because it felt damn good upon bashing my head in i stuck that damn movie in the <UNK> and watched it burn and that felt better than anything else i've ever done it took american psycho army of darkness and kill bill just to get over that crap i hate you sandler for actually going through with this and ruining a whole day of my life\n",
            "Negative\n",
            "<START> begins better than it ends funny that the russian submarine crew <UNK> all other actors it's like those scenes where documentary shots br br spoiler part the message <UNK> was contrary to the whole story it just does not <UNK> br br\n",
            "Negative\n",
            "<START> lavish production values and solid performances in this straightforward adaption of jane <UNK> satirical classic about the marriage game within and between the classes in <UNK> 18th century england northam and paltrow are a <UNK> mixture as friends who must pass through <UNK> and lies to discover that they love each other good humor is a <UNK> virtue which goes a long way towards explaining the <UNK> of the aged source material which has been toned down a bit in its harsh <UNK> i liked the look of the film and how shots were set up and i thought it didn't rely too much on <UNK> of head shots like most other films of the 80s and 90s do very good results\n",
            "Positive\n",
            "<START> the <UNK> tells the story of the four hamilton siblings teenager francis <UNK> <UNK> twins <UNK> joseph <UNK> <UNK> <UNK> <UNK> the <UNK> david samuel who is now the surrogate parent in charge the <UNK> move house a lot <UNK> is unsure why is unhappy with the way things are the fact that his brother's sister kidnap <UNK> murder people in the basement doesn't help relax or calm <UNK> nerves either francis <UNK> something just isn't right when he eventually finds out the truth things will never be the same again br br co written co produced directed by mitchell <UNK> phil <UNK> as the butcher brothers who's only other film director's credit so far is the april <UNK> day 2008 remake enough said this was one of the <UNK> to die <UNK> at the 2006 after dark <UNK> or whatever it's called in keeping with pretty much all the other's i've seen i thought the <UNK> was complete total utter crap i found the character's really poor very unlikable the slow moving story failed to capture my imagination or sustain my interest over it's 85 a half minute too long <UNK> minute duration the there's the awful twist at the end which had me laughing out loud there's this really big <UNK> build up to what's inside a <UNK> thing in the <UNK> basement it's eventually revealed to be a little boy with a teddy is that really supposed to scare us is that really supposed to shock us is that really something that is supposed to have us talking about it as the end credits roll is a harmless looking young boy the best <UNK> ending that the makers could come up with the boring plot <UNK> along it's never made clear where the <UNK> get all their money from to buy new houses since none of them seem to work except david in a <UNK> i doubt that pays much or why they haven't been caught before now the script tries to mix in every day drama with potent horror it just does a terrible job of combining the two to the extent that neither aspect is memorable or effective a really bad film that i am struggling to say anything good about br br despite being written directed by the extreme sounding butcher brothers there's no gore here there's a bit of blood splatter a few scenes of girls <UNK> up in a basement but nothing you couldn't do at home yourself with a bottle of <UNK> <UNK> a camcorder the film is neither scary since it's got a very middle class suburban setting there's zero atmosphere or mood there's a lesbian suggest incestuous kiss but the <UNK> is low on the exploitation scale there's not much here for the horror crowd br br filmed in <UNK> in california this has that modern low budget look about it it's not badly made but rather forgettable the acting by an unknown to me cast is nothing to write home about i can't say i ever felt anything for anyone br br the <UNK> commits the <UNK> sin of being both dull boring from which it never <UNK> add to that an ultra thin story no gore a rubbish ending character's who you don't give a toss about you have a film that did not impress me at all\n",
            "Negative\n",
            "<START> just got out and cannot believe what a brilliant documentary this is rarely do you walk out of a movie theater in such awe and <UNK> lately movies have become so over hyped that the thrill of discovering something truly special and unique rarely happens <UNK> <UNK> did this to me when it first came out and this movie is doing to me now i didn't know a thing about this before going into it and what a surprise if you hear the concept you might get the feeling that this is one of those <UNK> movies about an amazing triumph covered with over the top music and trying to have us fully convinced of what a great story it is telling but then not letting us in <UNK> this is not that movie the people tell the story this does such a good job of capturing every moment of their involvement while we enter their world and feel every second with them there is so much beyond the climb that makes everything they go through so much more tense touching the void was also a great doc about mountain climbing and showing the intensity in an engaging way but this film is much more of a human story i just saw it today but i will go and say that this is one of the best documentaries i have ever seen\n",
            "Positive\n",
            "<START> this movie has many problem associated with it that makes it come off like a low budget class project from someone in film school i have to give it credit on its <UNK> though many times throughout the movie i found myself laughing hysterically it was so bad at times that it was comical which made it a fun watch br br if you're looking for a low grade slasher movie with a twist of psychological horror and a dash of campy <UNK> then pop a bowl of popcorn invite some friends over and have some fun br br i agree with other comments that the sound is very bad dialog is next to impossible to follow much of the time and the soundtrack is kind of just there\n",
            "Negative\n",
            "<START> french horror cinema has seen something of a revival over the last couple of years with great films such as inside and <UNK> romance <UNK> on to the scene <UNK> <UNK> the revival just slightly but stands head and shoulders over most modern horror titles and is surely one of the best french horror films ever made <UNK> was obviously shot on a low budget but this is made up for in far more ways than one by the originality of the film and this in turn is <UNK> by the excellent writing and acting that ensure the film is a winner the plot focuses on two main ideas prison and black magic the central character is a man named <UNK> sent to prison for fraud he is put in a cell with three others the quietly insane <UNK> body building <UNK> marcus and his retarded boyfriend daisy after a short while in the cell together they stumble upon a hiding place in the wall that contains an old <UNK> after <UNK> part of it they soon realise its magical powers and realise they may be able to use it to break through the prison walls br br black magic is a very interesting topic and i'm actually quite surprised that there aren't more films based on it as there's so much scope for things to do with it it's fair to say that <UNK> makes the best of it's <UNK> as despite it's <UNK> the film never actually feels restrained and manages to flow well throughout director eric <UNK> provides a great atmosphere for the film the fact that most of it takes place inside the central prison cell <UNK> that the film feels very claustrophobic and this immensely benefits the central idea of the prisoners wanting to use magic to break out of the cell it's very easy to get behind them it's often said that the unknown is the thing that really <UNK> people and this film proves that as the director <UNK> that we can never really be sure of exactly what is round the corner and this helps to ensure that <UNK> actually does manage to be quite frightening the film is memorable for a lot of reasons outside the central plot the characters are all very interesting in their own way and the fact that the book itself almost takes on its own character is very well done anyone worried that the film won't deliver by the end won't be disappointed either as the ending both makes sense and manages to be quite horrifying overall <UNK> is a truly great horror film and one of the best of the decade highly recommended viewing\n",
            "Positive\n",
            "<START> when i rented this movie i had very low expectations but when i saw it i realized that the movie was less a lot less than what i expected the actors were bad the doctor's wife was one of the worst the story was so stupid it could work for a disney movie except for the murders but this one is not a comedy it is a laughable masterpiece of stupidity the title is well chosen except for one thing they could add stupid movie after dead husbands i give it 0 and a half out of 5\n",
            "Negative\n",
            "<START> i love cheesy horror flicks i don't care if the acting is sub par or whether the monsters look corny i liked this movie except for the <UNK> feeling all the way from the beginning of the film to the very end look i don't need a 10 page <UNK> or a sign with big letters explaining a plot to me but dark floors takes the what is this movie about thing to a whole new annoying level what is this movie about br br this isn't exceptionally scary or thrilling but if you have an hour and a half to kill and or you want to end up feeling frustrated and confused rent this winner\n",
            "Negative\n",
            "<START> anyone who could find redeeming value in this piece of crap ought to have their head examined we have the <UNK> heroin addicted part time hooker wife with <UNK> all over her body <UNK> received from repeated <UNK> by an abusive son now she is <UNK> breast milk all over the kitchen floor the release so gained somehow akin to helen <UNK> placing her hands in running water we have the husband who starts out by <UNK> a prostitute who just happens to be his daughter she's upset with him because he came too quickly and ends by murdering his female colleague having sex with her corpse and then <UNK> her up we have the kid who is relentlessly <UNK> by his classmates and who comes home and beats his mom you see it's all <UNK> deep huh the only decent moment in this horrendous pile of tripe is when the dad murders his son's <UNK> it's a good thing this turkey was shot on video because otherwise what a waste of expensive film it would be if that guy who thinks artists ought to be interested in this <UNK> is really serious no wonder most people think artists are insane we saw this lousy movie then put on zero woman the accused oh my god it was a <UNK> as to which one was worse what is going on in japan these days sick sick sick\n",
            "Negative\n",
            "<START> b movie at best sound effects are pretty good lame concept decent execution i suppose it's a rental br br you put some <UNK> oil in your mouth to save you from de poison <UNK> you cut de bite and suck out de <UNK> you gonna be ok tommy br br you stay by the <UNK> when agent harris calls you get me give me a fire <UNK> br br weapons we need weapons where's the <UNK> all we have is this <UNK> br br dr price is the snake expert br br local <UNK> can handle the occasional <UNK> alert every er in the <UNK> city area\n",
            "Negative\n",
            "<START> a total waste of time just throw in a few explosions non stop fighting exotic cars a deranged millionaire slow motion computer generated car crashes and last but not least a hugh <UNK> like character with wall to wall hot babes and mix in a <UNK> and you will have this sorry excuse for a movie i really got a laugh out of the dr evil like heavily <UNK> compound the plot was somewhere between preposterous and non existent how many <UNK> are willing to make a 25 million dollar bet on a car race answer 4 but didn't they become <UNK> through <UNK> responsibility this was written for <UNK> males it plays like a video game i did enjoy the <UNK> ii landing in the desert though\n",
            "Negative\n",
            "<START> laputa castle in the sky is the bomb the message is as strong as his newer works and more pure fantastic and flying pirates how could it be any better the art is totally amazing and the soundtrack which is <UNK> many times after this im not sure if this was the first time i heard it and evokes in me the most emotional sentimental response of any movie soundtrack <UNK> the female lead in this movie is totally awesome and the boy <UNK> is also a great role model he lives on his own the plot is classic miyazaki i won't give it away but the end is really great i rank this as one of miyazaki's three best with <UNK> and spirited away also you may want to check out <UNK> moving castle when it comes out sometime next year i hope if you like miyazaki check this one out as it readily available in the usa enjoy <UNK> a\n",
            "Positive\n",
            "<START> at the height of the <UNK> big <UNK> racism row in 2007 involving <UNK> <UNK> and the late <UNK> <UNK> i condemned on an internet forum those <UNK> b b ' fans who praised the show after years of bashing <UNK> <UNK> sitcoms such as <UNK> <UNK> <UNK> <UNK> <UNK> i thought they were being <UNK> and said so <UNK> ain't half hot <UNK> was then thrown into the argument with some pointing out it had starred an english actor <UNK> up well yes but michael bates had lived in india as a boy and spoke <UNK> <UNK> the show's <UNK> overlook the reality he brought to his performance as <UNK> <UNK> <UNK> the noted indian character actor <UNK> <UNK> said in a 1995 documentary <UNK> <UNK> the <UNK> that he was upset when he heard bates had landed the role but added no indian actor could have played that role as well as bates indeed br br <UNK> was perry and <UNK> companion show to <UNK> <UNK> also set in wartime the <UNK> english town of <UNK> on sea had been replaced by the hot steamy <UNK> of india in particularly a place called <UNK> where an army concert party puts on shows for the troops among them <UNK> <UNK> george <UNK> his first sitcom role since <UNK> in <UNK> camp <UNK> <UNK> <UNK> melvyn hayes <UNK> <UNK> <UNK> <UNK> <UNK> de <UNK> <UNK> graham john <UNK> and <UNK> <UNK> the late christopher mitchell <UNK> over this gang of <UNK> was the <UNK> sergeant major williams the brilliant <UNK> davies who regarded them all as <UNK> his frustration at not being able to lead his men up the jungle to engage the enemy in combat made him bitter and bullying though he was nice to <UNK> whom he thought was his <UNK> son then there was ever so english colonel reynolds donald <UNK> and <UNK> captain <UNK> michael <UNK> <UNK> was like a wise old <UNK> beginning each show by talking to the camera and closing them by <UNK> obscure <UNK> <UNK> he loved being <UNK> so much he came to regard himself as practically british his friends were the tea making <UNK> <UNK> the late <UNK> <UNK> who went on to <UNK> your <UNK> and the rope pulling <UNK> <UNK> <UNK> <UNK> so real indians featured in the show another point its <UNK> ignore <UNK> also provided what was described on the credits as <UNK> <UNK> similar to the <UNK> songs used as incidental music on <UNK> <UNK> each edition closed with him <UNK> <UNK> of hope <UNK> only to be <UNK> by a <UNK> up ' from williams the excellent opening theme was penned by jimmy perry and derek <UNK> br br though never quite <UNK> <UNK> <UNK> in the <UNK> affections <UNK> nevertheless was popular enough to run for a total of eight seasons in 1975 davies and <UNK> topped the <UNK> with a cover version of that old <UNK> <UNK> <UNK> they then recorded an entire album of old <UNK> entitled what else <UNK> <UNK> ' br br the show hit crisis point three years later when bates died of cancer rather than <UNK> the role of <UNK> the writers just let him be quietly forgotten when george <UNK> left the character of <UNK> took his place as <UNK> providing another source of comedy br br the last edition in 1981 saw the soldiers leave india by boat for <UNK> the <UNK> <UNK> watching them go with great sadness as did viewers br br repeats have been few and far between mainly on u k gold all because of its so called <UNK> reputation this is strange for one thing the show was not specifically about racism if a white man <UNK> up is so wrong why does david <UNK> 1984 film 'a passage to <UNK> still get shown on television it featured alec guinness as an indian and won two oscars it was derived from jimmy <UNK> own experiences some characters were based on real people the sergeant major really did refer to his men as <UNK> i take the view that if you are going to put history on television get it right <UNK> the past no matter how <UNK> it might seem to modern audiences is <UNK> <UNK> <UNK> was both funny and truthful and viewers saw this thank heavens for d v d 's i say time to stop this review as williams would say i'll have no <UNK> in this jungle\n",
            "Positive\n",
            "<START> i have only had the luxury of seeing this movie once when i was rather young so much of the movie is <UNK> in trying to remember it however i can say it was not as funny as a movie called killer tomatoes should have been and the most memorable things from this movie are the song and the scene with the elderly couple talking about poor timmy other than that the movie is really just scenes of little tomatoes and big tomatoes rolling around and people acting scared and overacting as people should do in a movie of this type however just having a very silly premise and a catchy theme song do not a good comedy make granted this movie is supposed to be a b movie nothing to be taken seriously however you should still make jokes that are funny and not try to <UNK> a mildly amusing premise into a full <UNK> movie perhaps a short would have been fine as the trailer showing the elderly couple mentioned above and a man desperately trying to gun down a larger <UNK> was actually pretty good the trailer itself looked like a mock trailer but no they indeed made a full movie and a rather weak one at that\n",
            "Negative\n",
            "<START> chances are is a charming romantic fantasy about a woman <UNK> shepherd whose husband christopher <UNK> is killed shortly after learning she is pregnant we then see the husband in heaven letting the powers that be know that he was taken too soon and that his wife needs him he is told he can return to earth but not as himself <UNK> 19 years where we see <UNK> daughter mary stuart masterson preparing to graduate from college and <UNK> a young man robert downey jr who it turns out is the reincarnation of her father the film is a little on the predictable side the story goes all the places you expect it to but it is so <UNK> played by an energetic cast especially shepherd and downey that you can't help but get wrapped up in the fun shepherd has rarely been seen on screen to better advantage and she and downey are backed by a talented group of character actors in supporting roles a lovely and charming fantasy that will <UNK> and <UNK> you\n",
            "Positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP0KC3eZi3sH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for i in range(20):\n",
        " # print(len(decoder(trainx[i])))\n",
        "\n",
        "# I will decide a size. All sentences longer than that will be chopped. \n",
        "# MAXSIZE = 256 words \n",
        "# Longer sentence -> first 256 words-> chop them up! \n",
        "# SHORTER sentences? -> PAD them up!\n",
        "# I am a good boy <PAD> <PAD> <PAD> <PAD> <PAD> \n",
        "# 1 2  3  4  5 0 0 0 0 0\n",
        "# y = w1*1 + w2*2... w5*5 + w6*0 + w7*0 ... w10*0 + bias\n",
        "# chopping_size = 10 words\n",
        "# y = w1*1 + w2*2..... w10* + bias \n",
        "# 2 or 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8YbFWSfplbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inbuilt keras method to do all this padding and chopping for us\n",
        "trainx_padded = keras.preprocessing.sequence.pad_sequences(trainx, value=0, padding='post', maxlen=256)\n",
        "#decoder(trainx_padded[3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vyb7QYJvrzjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for i in range(20):\n",
        "#  print(decoder(trainx_padded[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDOi7X0Hr_C2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HP_vocab = 10000\n",
        "HP_l1 = 16\n",
        "HP_l2 = 16\n",
        "HP_epochs = 50\n",
        "HP_maxlen = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVlKs3sThlUo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "c43058cf-2a33-409e-a1d1-242697a57851"
      },
      "source": [
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(HP_vocab, HP_l1))\n",
        "model.add(keras.layers.GlobalAveragePooling1D()) # no parameter! Average is fixed, cannot be manipulated\n",
        "# EVERY NEURAL NETWORK IS HARDCODED -> EVERY LAYER CAN ONLY DECIDE THE OUTPUT SIZE, NOT It's input size. \n",
        "# the only exception to above rule is INPUT_LAYER\n",
        "# EVERY OTHER LAYER HAS TO INPUT EVERYTHING GIVEN TO IT\n",
        "# Average Pooling has no learning, hence no new weights are calcualted on this layer\n",
        "model.add(keras.layers.Dense(HP_l2, activation=tf.nn.relu))\n",
        "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9tK-BN9hwBh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "582c0ca2-0680-4f9a-d00d-25a86a50f786"
      },
      "source": [
        "model.summary()\n",
        "# First Layer\n",
        "# Our Input Size was as follows:\n",
        "# First our sentence of 256 words is multiplied with our Dictionary matrix to arrive at 10,000X1 sized vector.\n",
        "# This vector is representation of your review-> \n",
        "# I am good <PAD>... <PAD>-> [ 0 0 0.1 0 0 0.2 0 0 0.3 0 0...0 ] # 10,000 elements\n",
        "# I am bad <PAD>... <PAD>-> [ 0 0 0.1 0 0 0.2 0 0.4 0 0 0...0] # 10,000 elements \n",
        "# Embedding layer will further break it down into N datapoints? why so? -> so that finer vectors can be created\n",
        "# this is how normalization of vectors is handled \n",
        "# No. of parameters generated will be: \n",
        "# 16 type of vectors X 10,000 dictionary_words\n",
        "# total_parameters_learnt = 16 X 10,000 = 160,000\n",
        "# 2nd layer is Average Pooling-> simple math operation, nothing new learnt, hence no wts calculated\n",
        "# 3rd layer -> input X output + bias -> 16 X 16 + 16 = 272\n",
        "# 4th layer -> input X output + bias -> 16 X 1 + 1 = 17"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 16)          160000    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,289\n",
            "Trainable params: 160,289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvioaZZ-tBjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u4QMc_snZ21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}