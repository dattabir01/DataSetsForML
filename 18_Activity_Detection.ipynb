{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "18 Activity Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNn3N3O6/pCPIdnKcyhAh8z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-forty-two/DataSetsForML/blob/master/18_Activity_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7PUz0znBOfA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we will use a hardcoded video as test data\n",
        "# if doing on laptop you can even use webcab for same program \n",
        "# as input stream \n",
        "\n",
        "# inputs were hardcoded,video is not a user input but an mp4 or avi file\n",
        "\n",
        "# Algorithm\n",
        "# 1. Input a video (or live stream)\n",
        "# 2. open the frames as images\n",
        "# 3. Run object detection on images to understand continous usage of objects (RESNET/VGG)\n",
        "# 4. Continous usage of objects is called ACTIVITY\n",
        "# 5. in each frame of video, classify the frame as an activity\n",
        "# 6. combine the frames back into a video to deliver as classified output! \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd5QlZ0FEjHs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######## start of: TRAIN.py\n",
        "\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import pickle # to read/write binary files as dumps (for weights and biases or binarized labels)\n",
        "import cv2\n",
        "import os # os.separators to separate path names and get file name and file folder name\n",
        "from keras.applications import ResNet50\n",
        "# to customize models with own layers\n",
        "from keras.layers.core import Flatten, Dropout, Dense\n",
        "from keras.layers.pooling import AveragePooling2D # AVERAGE pixel -> do this when color of object shouldn't matter \n",
        "from keras.layers import Input # input layer can take general or any input size (dynamic input size but once decalred cannot be changed)\n",
        "from keras.models import Model\n",
        "#label binarizer to encode/decode binarized outputs \n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import imutils\n",
        "from imutils import paths\n",
        "# but what if i wanted to take average without losing channels?\n",
        "# Conv2D -> GlobalAveragePooling-> Conv2D would split data into R, G and B channels \n",
        "# then GAP will calculate averages for R, G, and B separately thus preserving colors \n",
        "from keras.optimizers import SGD\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xU9AiEvE4Qn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# variables that should have been collected from user through UI or command line or API post\n",
        "dataset = 'data'\n",
        "model_path = 'bin'\n",
        "binarizer_path = 'bin'\n",
        "evaluation_path = 'eval'\n",
        "test_path = 'test'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heEmQnzIHa_T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "faac0cc1-1d24-4279-d725-96f9b2687975"
      },
      "source": [
        "# shuffle the data after reading all paths\n",
        "import random\n",
        "imagePaths = sorted(list( paths.list_images(dataset)))\n",
        "random.seed(42)\n",
        "random.shuffle(imagePaths)\n",
        "imagePaths[:2]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data/hockey/00000065.jpg', 'data/weightlifting/00000072.jpg']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-R3or1VFHsVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# need to conver to img_to_array and resize \n",
        "# this is how to use ResNET \n",
        "# in our VGG-> we took (96,96,3)\n",
        "# larger images are required because smaller image= not enough filters possible\n",
        "label_names = [\"weightlifting\",\"swimming\",\"hockey\",\"basketball\"]\n",
        "labels = []\n",
        "data = []\n",
        "for path in imagePaths:\n",
        "  label = path.split(os.path.sep)[-2] # folder name! to represent class name \n",
        "  if label not in label_names:\n",
        "    continue\n",
        "  labels.append(label)\n",
        "  img = cv2.imread(path)\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)   # these are simple reshape, resize and expand_dims functions\n",
        "  img = cv2.resize(img, (224,224)) \n",
        "  data.append(img)\n",
        "  # RESNET model -> input size (224,224,3) , only accept RGB channel, while most images\n",
        "  # like jpeg, png, jpg, giff are arranged in BGR (just opposite)\n",
        "  # Object detection -> there order of the color did  not matter because RGB values combined would have \n",
        "  # resulted in same color anyway \n",
        "  # just in case error handling-> if image is already RGB, the below function will do nothing, if its BGR, it will\n",
        "  # be converted to RGB \n",
        "  \n",
        "  # they readjust dimensions to fit various frameworks \n",
        "  # RESNET doesn't require image aug sep because it was inbuilt into its architecture \n",
        "  # aug.flow-> model.fit_generator \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gmybBVnJ_-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "data = np.array(data)\n",
        "labels = np.array(labels)\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "labels # HOT-encoded MATRIX \n",
        "(xtrain, xtest, ytrain, ytest) = train_test_split(data, labels, test_size=0.2, stratify=labels, random_state=42 )\n",
        "# << left shift operation\n",
        "#       0 0 0 1  0 0 0 0   -> left shift by 2 - >   0 1 0 0 0 0 0 0 \n",
        "#          -> right shift by 1 -> 0000 1000 \n",
        "# This label binarizing is EXTREMELY FAST because of LEFT shift operations which happen at bit-level \n",
        "# there is no need to go through the entire list of items, just need to know HOW many items"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aath-DKpKGXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split into train/test\n",
        "# whenever we need classes balanced while splitting, we use a process called 'stratification'\n",
        "# it shuffles on the basis of types of output labels, and not just random_state\n",
        "(xtrain, xtest, ytrain, ytest) = train_test_split(data, labels, test_size=0.2, stratify=labels, random_state=42 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZXaD49nMPXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainAug = ImageDataGenerator(rotation_range=25, zoom_range=0.20, width_shift_range=0.2, height_shift_range=0.2, \n",
        "                              shear_range=0.20, horizontal_flip=True, fill_mode='nearest')\n",
        "\n",
        "valAug = ImageDataGenerator() # default augmentation (everything is set to false-> no augmentation)\n",
        "# this will help validate the model in 2 augmentation- one with our v/s with any other option! \n",
        "\n",
        "means = np.array( [ 103.939, 116.779, 123.68 ])\n",
        "trainAug.mean = means\n",
        "valAug.mean = means \n",
        "# deviations will be calculated from respective means\n",
        "# train data -> trainaug.means\n",
        "# valdata -> valAug.means \n",
        "\n",
        "# Since the data is augmented and no longer the original image, the difference between the original image (no aug- val)\n",
        "# and augmented image (trainAug) can be calculated as difference in the mean \n",
        "# when using keras.application \n",
        "# To find out this mean there are various formula, but we are going to use the preferred production method-\n",
        "# WITH RESNET and VGG, we use means from 'ImageNet'. \n",
        "# 3 averages-> 1 each for R, G and B \n",
        "# From scratch way is to calculate it based on your data. BUT, when doing transfer learning, follow what model\n",
        "# says. VGG and Restnet for 'ImageNet' standards. \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUih1xP-UK4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LAST time-> we built our own PandaVGG, THIS time-> transfer learning using ResNet \n",
        "# Last time-> we calculated our weights for PandaVGG, THIS TIME-> preserved ResNet's weights by setting \n",
        "# non trainable layers \n",
        "# REST of it is EXACTLY the same except hyperparameters \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# cloud -> hadoop (parquet) -> g(s3) -> no bigquery\n",
        "# PySpark notebooks -> external tables -> DataBricks \n",
        "# Transfer learning and build our model on top of it\n",
        "basemodel = ResNet50(weights='imagenet', include_top=False, input_tensor=Input(shape=(224,224,3)))\n",
        "# activity detection and classification model\n",
        "# use object detection as base algorithm and build your own algo on top of it \n",
        "# basemodel was loaded as KNOWLEDGE\n",
        "myModel = basemodel.output # TRANSFER LEARNING (Knowledge transfer) is done \n",
        "# another of adding layers besides model.add\n",
        "# MATMUL way -> multiplicative way of adding layers (because anyway between layers its the mul, inside layers-> add)\n",
        "# model  = (additive_layers)(model)\n",
        "myModel = AveragePooling2D(pool_size=(7,7))(myModel) # in matmul AXB != BXA\n",
        "#Pattern recognition \n",
        "# till now my data is 2 d! Image Pattern Recog\n",
        "# Flatten -> Dense(ReLu) -> Dense (Softmax)\n",
        "myModel = Flatten(name='flatten')(myModel) # specific names to layers, you can do so by name property\n",
        "myModel = Dense(512, activation='relu')(myModel)\n",
        "myModel = Dense(len(lb.classes_), activation='softmax')(myModel)\n",
        "\n",
        "model = Model(inputs=baseModel.input, outputs=myModel) # connection of the two pipes -> ResNet and Mymodel \n",
        "\n",
        "for layer in basemodel.layers:\n",
        "  layers.trainable = False \n",
        "# Learning will be ONLY for myModel not for basemodel-> resnet weights will be preserved, while myModel will keep \n",
        "# learning \n",
        "# PARTIAL LEARNING-> some part of model is already smart, some part is learning from it's smartness \n",
        "\n",
        "\n",
        "\n",
        "# IF NOW YOU WANT MODEL TO BE FAMILIAR WITH YOUR IMAGES, you need to show images again!\n",
        "# but this brings us a problem- if i retain the model -> then the imagenet weights will be recalculated!\n",
        "\n",
        "# every time back-prop -> weights and bias are recalculated\n",
        "# everything that i inherited from resnet will GO AWAY except its shape!!\n",
        "# TO PRESERVE THE WEIGHTS INHERITED you can set that layers will NOT be learnt upon\n",
        "# thAT means their parameters will be unaffected by gradient descent \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}