{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "17 Test Prep.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPbVjHQCc3PE7lf4f6xm0ql",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-forty-two/DataSetsForML/blob/master/17_Test_Prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cSqZyhHYnSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feedback for the training please\n",
        "# https://tcheck.co/HgPG96 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUFDFOJyZIjV",
        "colab_type": "text"
      },
      "source": [
        "Topics: From last saturday, till yesterday (no CNN)\n",
        "\n",
        "RNN, NLP, Model structuring and deployment\n",
        "\n",
        "Other topics:\n",
        "SARIMAX, \n",
        "\n",
        "\n",
        "A particular inside model can be checked via eager evaluation, but training an entire model with eager is NOT recommended- because NO model (graph) is actually created. It can only be used to evaluate and verify results of variables/constants/placeholders created. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkA2nnXlZHPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LABEL ENCODING (words are not math related to each other)-> sentiment analysis, text-to-speech, encryption\n",
        "# One-HOt Encoding -> CATEGORICAL COLUMN is broken into multiple columns that will be 1-0 encoded\n",
        "# Multi-hot encoding\n",
        "# Word 2 Vector (PREDICTION) -> Word guessing next character, next word, autocomplete, distance between words\n",
        "# Label Binarizing \n",
        "\n",
        "# Label Encoding can be used anywhere where STRINGS are NOT expected to have a math relationship or sequence attached\n",
        "# BETWEEN THE WORDS \n",
        "\n",
        "# One-Hot encoding \n",
        "#Profit City\n",
        "#10      B 1\n",
        "#20      M 2\n",
        "#30      M 2\n",
        "#15      C 3\n",
        "# y = weight1*Profit + weight2*City + bias -> did I mean to say that Mumbai / Bangalore = 2? Chennai - Bangalore= Mum?\n",
        "# The above is LACK of statistical relationship and LABEL encoding was NOT the way\n",
        "# 1-Hot encoding \n",
        "# Profit City_B   City_M    City_C\n",
        "#  10       1      0        0\n",
        "#  20       0      1        0\n",
        "#  30       0      1        0\n",
        "#  15       0      0        1\n",
        "\n",
        "# Label Binarizers are used to represent OUTPUT in one-hot matrix style  \n",
        "# B -> [[1,0,0],[0,0,0],[0,0,0]] # ASSUME\n",
        "# M -> [[0,0,0],[0,0,1],[0,0,0]]\n",
        "# C -> [[0,0,0],[0,0,0],[0,1,0]]\n",
        "\n",
        "# Diff between One-Hot Encoding and Binarizer -> One hot is for INPUT, binarizer is for OUTPUT \n",
        "# why a matrix? -> To represent multiple output categories if REQUIRED \n",
        "\n",
        "\n",
        "\n",
        "# Multi-Hot encoding, not just represent multiple categories, but now you have representation for data that doesn't\n",
        "# belong to ANY category [ 0 0 0]\n",
        "# MULti-Hot Encoding (more than one column is marked Hot)\n",
        "# Item               item_available_Mumbai item_available_chennai   item_available_madurai\n",
        "#  Pop's sickle               1                        1                   0\n",
        "#  Hankerchief                0                        1                   1\n",
        "#  Umbrella                   0                        0                   0\n",
        "\n",
        "\n",
        "\n",
        "# For an equation relation to Bangalore, y = w1*Profit + w2*Bangalore + w3*Mumbai + w4*chennai + bias\n",
        "#  y = w1*profit + w2 *1 + w3*0 + w4*0 <- the weights not related to BANGALORE automatically became 0 thanks to\n",
        "# one hot encoding -> CATEGORICAL DATA IS HANDLED this way \n",
        "\n",
        "# Humans -> English, NN-> number \n",
        "# Human i/p is convrt to NN input by word_to_idx\n",
        "# NN's output is numbers. Need to convrt them to words so that they are human readable!\n",
        "# for that idx_to_word dictionary is used\n",
        "\n",
        "# Word2Vector is a type of ENCODING generated with help of Neural Networks (to convert strings to numbers)\n",
        "# LSTM -> Long Short-Term Memory is a series of layers of Neural Network that are capable of remembering, forgetting\n",
        "# and predicting SEQUENTIAL DATA (TIME SERIES, ANOMALIES, NEXT WORD)\n",
        "\n",
        "# Word2Vec can be used to generate string vectors that can be passed to LSTM\n",
        "\n",
        "# every other format was simple-> either words were counted, ranked (Label Encoder)\n",
        "# but GUESSING the next word required more than just context \n",
        "\n",
        "# LISTEN and SILENT -> if i was char by char encoding -> both would have resulted in same encoding even though \n",
        "# they have very diff meanings!!! that is why, words were preferred over character \n",
        "\n",
        "# SEQUENCE -> character by character sequence was better \n",
        "\n",
        "\n",
        "# instead of being a straightforward linear equation, now PROBABILITY plays a role \n",
        "\n",
        "# p_fn(next_word) = weight*p(previous_word) + bias \n",
        "# Now suppose a previous word has been provided. In that case, p(previous_word) = 1! it exists!\n",
        "\n",
        "# p_fn(next_word) = weight * 1 + bias = weight + bias <- this becomes a straight forward math calculation\n",
        "# that a NEURAL NETWORK COULD HAVE ALSO GIVEN ME!!!\n",
        "\n",
        "# 1. Train a NN to predict character by character sequence\n",
        "# 2. Extract the weights of Hidden Layer\n",
        "# 3. Treat these weights as VECTORS!!! \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH21EP1xsXNM",
        "colab_type": "text"
      },
      "source": [
        "RNNs -> Time Series -> architecture and order of data! \n",
        "\n",
        "Vanishing Gradient: Challenges of RNN because of which we don't use them- instead we rely on improvements like LSTM and GRU! \n",
        "\n",
        "Why does VG happen? because RNN take all the history available into context. For example, Suppose we had temperature from last 1000 years. Then temp tomorrow was surely dependent on today, maybe last week, but definitely not from 1600 AD!!\n",
        "\n",
        "But RNNs will consume this history and still decide that YES, tomorrow temp is dependent on even 13th century!! this leads to miscalculated mu, sigma and surreal results. \n",
        "\n",
        "Hence FORGET gate was introduced. Idea was that using LogSoftmax we can choose to 'forget' or 'keep' only a piece of info. Older info (because of scaling to SIGMOID) starts tending to 0, and in subsequent epochs, becomes 0. \n",
        "\n",
        "\n",
        "Tanh scales and brings everything into -1 to 1 with average at 0. So, first normalize, and then forget. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_cQvKu9tnqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkzbssIttnnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxSCsi4tvBa1",
        "colab_type": "text"
      },
      "source": [
        "Where should the model run?\n",
        "\n",
        "Is it a hospital bed where a patient with unique condition needs to be monitored?\n",
        "AI needs to run directly into the DEVICE! certain actions like alerts should happen immedaitely!\n",
        "- EDGE COMPUTING - Field Programmable Gate Arrays \n",
        "\n",
        "It is a factory were 100s of robotic arms have to coordinate?\n",
        "- all arms have same function- to COORDINATE\n",
        "- a typical server-client architecture was good!\n",
        "- Robotics arms could be client, a central AI web server!\n",
        "- because this is a factory, you cannot have outages!\n",
        "- you need reliability, integrity! \n",
        "- Need a central SERVER or SERVICE that could take care of running at scale with multiple machine to handle FAULTs and FAILURES as well\n",
        "- Container based deployment using KUBERNETES (manager of clusters working as servers) \n",
        "\n",
        "\n",
        "Is it a security camera on a conveyer belt trying to classify heavy objects and remove them from belt?\n",
        "- individual item, not a cluster of items! \n",
        "- and if 1 camera is monitoring 1 conveyer belt, then all i need is a dedicated AI model that could have served that camera?\n",
        "- Container based DOCKER deployment (DOCKER-> A SINGLE node machine) \n",
        "- machine WILL restart, crash, update, party and so on. \n",
        "- SHOULD be avoided for production, good for dev/test \n",
        "- Can be used in production if user group is small or work timings are fixed between particular hours only (that is we can plan and expect the restart)\n",
        "\n",
        "\n",
        "Are you a chat bot trying to interact with user?\n",
        "- Web API! where a web client could have made HTTPs calls to my backend with messages and my central chatbot server could have responded!\n",
        "- FLASK or any similar API framework \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGK-ROC3ZzUt",
        "colab_type": "text"
      },
      "source": [
        "Q-model performance of these algos\n",
        "Answer-\n",
        "use MSE, MAE, Acc, Precision, Recall, F1 score, AUC \n",
        "measure whatever is applicable\n",
        "and then decide performance - select whatever top-few algos give you best performance\n",
        "then use those ALGOS for a period of time (usually depends on data, but 1-2 years is a common practice)\n",
        "by that time, they will be able to tell you that in LONG run which of them was better. For this, it is important that they are updated with new training data EQUALLY and NO DISCRIMINITATION in HARDWARE running those algos\n"
      ]
    }
  ]
}